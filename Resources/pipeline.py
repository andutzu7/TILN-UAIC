import zeep 
import re

def request_lemmatized(text):

    wsdl = 'http://nlptools.info.uaic.ro/WebPosRo/PosTaggerRoWS?wsdl'
    client = zeep.Client(wsdl=wsdl)
    response = ''
    response += client.service.parseSentence_XML(text)
    response += '游뱔'
    return response


if __name__ == '__main__':
# Adding the stopwords to the list
    stopwerdz = []
    count = 0
    with open('stopwords.txt',mode = 'r') as stopwerds:
        while 1:
            werd = stopwerds.readline()
            werd = werd.strip('\n')
            stopwerdz.append(werd)
            count = count + 1
            if count > 433:
                break
            
    text = request_lemmatized("Regina Elisabeta a II-a a Marii Britanii a fost nevoit캒 s캒 stea singur캒 칥n timpul funeraliilor, din cauza restric콖iilor impuse de pandemia de coronavirus. Camerele de filmare nu au putut capta expresia chipului s캒u, regina a stat mai mult cu capul plecat.Al콖i membri ai casei regale care au ap캒rut 칥n imagini aveau fe콖ele 칥ntristate. Prin콖ul Edward, cel mai mic dintre copiii reginei 를 ai prin콖ului Philip, a fost v캒zut pun칙ndu-를 m칙na pe frunte 칥n timp ce corul intona imnuri religioase. El a fost v캒zut apoi fix칙nd de la distan콖캒 sicriul tat캒lui s캒u.Fiecare membru al casei regale - mai ales cei special invita콖i - au purtat haine de doliu de culoare 칥nchis캒 를 m캒릆i sanitare negre.Ei au p캒strat distan콖a unii de ceilal탵i 탳i 칥n a탳ezarea pe scaunele din interiorul capelei St.George, pentru a respecta restric콖iile impuse de pandemia de coronavirus.")
    arr = text.split('\n')
    arr_lemmas =[]
    final_tokenized = []
    index = 0
    for line in arr:
        found = re.search('.*(LEMMA="[A-Za-z캒칙칥탳탵1-9]*").*',line)
        if found:
            res = found.group(1)
            arr_lemmas.append(res)
        found = re.search('游뱔',line)
        if found:
            arr_lemmas.append('游뱔')
    for lemma in arr_lemmas:
        found = re.search('LEMMA="([A-Za-z캒칙칥탳탵1-9]*)".*',lemma)
        if found:
            res = found.group(1)
            final_tokenized.append(res)
            index = index +1
        if index == len(arr_lemmas)-1:
            found = re.search('游뱔',line)
            if found:
                final_tokenized.append('游뱔')
            break

    print(final_tokenized)